---
title: 统计学习学习记录
tags:
  - 机器学习
abbrlink: e5e13074
date: 2024-02-21 19:30:58
---

作为非科班出身的金数人，在学习机器学习模型时我发现自对一些最基础、最本质的概念理解不够深入，所以打算写一篇blog，记录一下自己在学习的时候遇到的零零散散的问题。

## 1.极大似然估计(MLE)和损失函数的关系

1. 若变量服从高斯分布，MLE的结果是OLS
2. 若变量服从伯努利分布，MLE的结果是logistic回归
3. 若变量服从多项式分布，MLE的结果是softmax

## 2.当我们对损失函数正则化时，我们在做什么？

做正则化的目的就是为了提高训练出模型的泛化能力。影响模型泛化能力的是权重 $w$ 和偏置 $b$ 。

参考：[“L1和L2正则化”直观理解](https://www.bilibili.com/video/BV1Z44y147xA/?spm_id_from=333.337.search-card.all.click&vd_source=44ba9a7b92cb9c058705d88870afca92)

### 2.1拉格朗日对偶角度

所谓正则化，就是在损失函数中加上Lp范数。

![image-20240221221538595](./统计学习学习记录/image-20240221221538595.png)

$x$是训练集里面的数据，视作常数不考虑；真正决定拟合效果的是权重$W$，所以偏置$b$虽然是未知参数也不进行研究；
$$
\|W\|_1-C \leq \mathbf{0}
$$
这里可以理解为只考虑对$W$的可行域范围进行约束 。

> ![Lp范数的可视化](./统计学习学习记录/image-20240224212331595.png)
>
> 只有p>=1的时候，Lp范数才是一个凸集，这时候以Lp范数作为约束的问题才能是一个凸优化问题；反之，若0<p<1，集合就是非凸集。

 **我们看到的L2正则化的函数和原问题的拉格朗日函数之间的关系**

![image-20240224213614031](./统计学习学习记录/image-20240224213614031.png)

在这个问题中，红的的函数与绿色相比，少了一个$\lambda$$C$，这个C其实调节的是圆半径的大小，在绿色函数中，就是C视为已知，来求$\lambda$；而在红色函数中，是用$\lambda$来调节圆的半径，每一个极值点都对应一对共线相反大小相等的梯度，对应一个特别的$\lambda$，也就是对应一个半径C（即可以通过调节$\lambda$来调节$C$。

红色和绿色两个函数对$W$求梯度后，得到的W值是相等的，因为$\lambda$$C$在对$W$求导的时候为0。看似这两个问题是等价的，但是其实超参数已经改变了，上面的超参数是$C$，下面的是$\lambda$。

**L1正则化和L2正则化的特性**：

L1正则化会带来稀疏性。（待补充）

![image-20240221223742439](./统计学习学习记录/image-20240221223742439.png)

> 为什么说L1正则化（右）可以带来稀疏性，就是因为L1正则化后的极值点容易出现在坐标轴上，而出现在坐标轴上意味着其他某些维度的值为0，比如要用胡子和毛色区分猫咪和狗，L2正则化（左）可能只是赋予两个特征不同的权重，$\lambda$的作用是来调节权重大小，而L1正则化就可能只考虑胡子，而不考虑毛色，这就带来了**稀疏性**。

